from enum import Enum
from typing import List, Union, Generator, Literal, Optional

from pydantic import BaseModel

from aily_code_sdk_core import action

ACTION_API_NAME = 'action:brn:cn:spring:all:all:connector_action_runtime:/spring_sdk_llm'


class LLMModel(Enum):
    BYOM_PRO = "BYOM-pro"
    BYOM_ULTRA = "BYOM-ultra"
    FOUR_TURBO = "4-turbo"


class Function(BaseModel):
    name: str
    description: str
    parameters: Optional[str]


class MessageToolCall(BaseModel):
    id: str
    """The ID of the tool call."""
    function: Function
    """The function that the model called."""
    type: Literal["function"]
    """The type of the tool. Currently, only `function` is supported."""


class Usage(BaseModel):
    input_tokens: int
    """The number of input tokens which were used."""

    output_tokens: int
    """The number of output tokens which were used."""


class Message(BaseModel):
    content: Optional[str] = None
    role: Literal["assistant", "user", "system"]
    """The role of the author of this message."""
    # tool_calls: Optional[List[MessageToolCall]] = None
    # """The tool calls generated by the model, such as function calls."""
    # function_call: Optional[dict] = None
    # """The function call generated by the model."""
    usage: Optional[Usage] = None


def generate(
        messages: Union[List[Message], List[dict[str, str]]],
        model: Union[
            str,
            Literal[
                "BYOM-lite",
                "BYOM-plus",
                "BYOM-pro",
                "BYOM-max",
                "BYOM-ultra",
                "BYOM-4o",
                "BYOM-embedding",
                "3.5-Turbo",
                "3.5-Turbo-16K",
                "4-8K",
                "4-32k",
                "4-Turbo",
                "4o",
            ],
        ],
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        # stream: bool = False,  # 当前并不支持
        # tools: Optional[List[Function]] = None,
        # tool_choice: Literal["none", "auto", "required"] = "none",
        timeout: Optional[float] = None,
) -> Message:
    """
    Generates a response message based on the input messages and parameters.

    Args:
        messages: The list of input messages.
        model: The LLM model to use for generation.
        max_tokens: The maximum number of tokens to generate.
        temperature: The temperature value for generation.
        # stream: Whether to stream the response. Currently not supported.
        # tools: The tools available for the model to use.
        # tool_choice: The choice of tool usage. Can be "none", "auto", or "required".
        timeout: The timeout value for the API request.

    Returns:
        The generated response message.
    """
    model_value = model.value if isinstance(model, LLMModel) else model
    if isinstance(messages[0], dict):
        messages = [Message(role=message["role"], content=message["content"]) for message in messages]

    action_data = {
        "llmID": model_value,
        "chatCompletionParameters": {
            "ChatCompletionMessages": [
                {
                    "Role": message.role,
                    "Content": message.content,
                }
                for message in messages
            ],
            "MaxTokens": max_tokens,
            "Temperature": temperature,
            # "Stream": stream,
        },
    }

    # if tools:
    #     action_data["chatCompletionParameters"]["FunctionDefinitions"] = [
    #         {
    #             "Name": tool.name,
    #             "Description": tool.description,
    #             "Parameters": tool.parameters,
    #         }
    #         for tool in tools
    #     ]

    # if tool_choice == "required":
    #     action_data["chatCompletionParameters"]["FunctionCall"] = '{"name": "extract_entity_tool"}'
    #     action_data["chatCompletionParameters"]["ToolChoice"] = tool_choice

    res = action.call_action(
        action_api_name=ACTION_API_NAME,
        action_data=action_data,
    )

    if res["Choices"]:
        choice = res["Choices"][0]
        content = choice["Message"].get("Content")
        role = choice["Message"]["Role"]
        # function_call = choice["Message"].get("FunctionCall")
        # tool_calls = [
        #     MessageToolCall(
        #         id=tool_call["ID"],
        #         function=Function(
        #             name=tool_call["Function"]["Name"],
        #             parameters=tool_call["Function"]["Arguments"],
        #             description=""
        #         ),
        #         type="function",
        #     )
        #     for tool_call in choice["Message"].get("ToolCalls", [])
        # ]
        return Message(
            content=content,
            role=role,
            # function_call=function_call,
            # tool_calls=tool_calls if tool_calls else None,
            usage=Usage(
                input_tokens=res.get('Usage', {}).get('PromptTokens'),
                output_tokens=res.get('Usage', {}).get('CompletionTokens'),
            )
        )
    else:
        return Message()
