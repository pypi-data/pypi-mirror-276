# ----------------------------------------------------------
# Storage Port
# ----------------------------------------------------------
import asyncio
import os
import pickle
import re
import time
import sys
import traceback
from typing import Dict, List
from multiprocessing import Process
import yaml
from typing import Dict
from logging import ERROR, getLogger

# from surrealdb import Surreal
from surrealist import Surreal as Surrealist

# from surrealist.connections.connection import logger as surreal_logger
from .exceptions import BadData

# TODO: reallocate
UID = str
WAVE = float

# ---------------------------------------------------------
# Loggers
# ---------------------------------------------------------

from agptools.logs import logger
from agptools.helpers import parse_uri, build_uri
from agptools.containers import merge as merge_dict

# from 0.5.2 surreslist uses root default level
for _ in (
    "surrealist.connection",
    "surrealist.connections.websocket",
):
    getLogger(_).setLevel(ERROR)


log = logger(__name__)

from .definitions import (
    FORCE_SAVE,
    MONOTONIC_KEY,
    ORG_KEY,
    filter_4_compare,
    build_fqid,
)
from .helpers import expandpath
from .model import BaseModel


# REGEXP_FQUI = re.compile(r"((?P<ns>[^/]*?)/)?(?P<table>[^:]+):(?P<uid>.*)$")
def split_fqui(fqid):
    try:
        table, uid = fqid.split(":")
        return table, uid
    except ValueError:
        return fqid, None


class iCRUD:
    MODE_SNAPSHOT = "snap"
    MODE_TUBE = "tube"

    async def create(self, item):
        "TBD"

    async def read(self, fqid):
        "alias for get"
        return await self.get(fqid)

    async def update(self, fqid, item):
        "TBD"

    async def delete(self, fqid):
        "TBD"

    async def put(self, fqid, data: Dict, mode=None) -> bool:
        "Try to create / update an item of `type_` class from raw data"

    async def list(self):
        "TBD"

    async def count(self, table):
        "TBD"

    async def exists(self, fqid):
        "TBD"

    async def get(self, fqid, kind=None) -> BaseModel | None:
        "TBD"
        pass

    async def set(self, fqid, data: Dict, merge=False):
        "TBD"
        return False

    async def put(self, fqid, data: Dict, mode=None) -> bool:
        return await self.set(fqid, data)

    async def get_all(self):
        "TBD"

    async def set_all(self, items):
        "TBD"

    async def delete_all(self):
        "TBD"

    async def exists_all(self, uids):
        "TBD"

    async def get_many(self, uids):
        "TBD"

    async def set_many(self, uids, items):
        "TBD"

    async def delete_many(self, uids):
        "TBD"

    async def exists_many(self, uids):
        "TBD"

    async def save(self, nice=False, wait=False):
        "TBD"

    async def info(self):
        "TBD"
        return {}


class iPolicy:
    "base of criteria / policies"
    DISCARD = "discard"
    STORE = "store"

    def __init__(self, storage):
        self.storage = storage

    async def action(self, mode, thing, data):
        return self.STORE


class DataInsertionPolicy(iPolicy):
    "when a record must be inserted or not"

    async def action(self, mode, thing, data):
        if mode in (iCRUD.MODE_SNAPSHOT,):
            return self.STORE

        if mode in (iCRUD.MODE_TUBE,):
            # check if last data is the same but MONOTONIC_KEY
            tube_name = thing.split(":")[0]
            fquid = f"{TUBE_WAVE}:{tube_name}"

            # TODO: use sync or cache for faster execution
            last = await self.storage.last_wave(fquid)
            _last = filter_4_compare(last)
            _data = filter_4_compare(data)
            if _last == _data:
                return self.DISCARD

            return self.STORE

        return self.DISCARD


class Storage(iCRUD):
    def __init__(
        self, url, mode=iCRUD.MODE_SNAPSHOT, policy=DataInsertionPolicy
    ):
        super().__init__()
        self.url = url
        self.mode = mode
        self.policy = policy(storage=self)
        self.background = []

    def running(self):
        self.background = [p for p in self.background if p.is_alive()]
        return len(self.background)

    async def info(self):
        raise NotImplementedError()


class StoragePort(Storage):
    PATH_TEMPLATE = "{self.url}/{table}"

    def __init__(self, url="./db", mode=iCRUD.MODE_SNAPSHOT):
        super().__init__(url=url, mode=mode)
        url = expandpath(url)
        if not os.path.exists(url):
            os.makedirs(url, exist_ok=True)
        self.url = url
        self.cache = {}
        self._dirty = {}

    def _file(self, table):
        return self.PATH_TEMPLATE.format_map(locals())

    def load(self, table, force=False):
        universe = self.cache.get(table)
        if force or universe is None:
            path = self._file(table)
            if os.path.exists(path):
                try:
                    universe = self._real_load(path)
                except Exception as why:
                    log.warning(why)
            if universe is None:
                universe = {}
            self.cache[table] = universe
        return universe

    _load = load

    def _save(self, table, universe=None, pause=0, force=False):
        if self._dirty.pop(table, force):
            if universe is None:
                universe = self.load(table)
            path = self._file(table)
            self._real_save(path, universe, pause=pause)

    def _real_load(self, path):
        raise NotImplementedError()

    def _real_save(self, path, universe, pause=0):
        raise NotImplementedError()

    async def get(self, fqid, query=None, **params):
        table, uid = split_fqui(fqid)
        universe = self.load(table)
        if query:
            raise NotImplementedError

        data = universe.get(fqid, {})
        return data

    async def set(self, fqid, data, merge=False):
        table, uid = split_fqui(fqid)
        universe = self.load(table)
        if merge:
            data0 = await self.get(fqid)
            # data = {** data0, ** data} # TODO: is faster?
            data0.update(data)
            data = data0

        universe[fqid] = data
        self._dirty[table] = True
        return True

    async def save(self, table=None, nice=False, wait=False):
        table = table or list(self.cache)
        if not isinstance(table, list):
            table = [table]
        for i, tab in enumerate(table):
            pause = i if nice else 0
            self._save(tab, pause=pause + 0.1)

        log.info("waiting data to be saved")
        while wait and self.running() > 0:
            await asyncio.sleep(0.1)
        return self.running() == 0

    async def info(self, ns=""):
        "Returns storage info: *tables*, etc"
        if ns:
            table = f"{ns}/.*"
        else:
            table = f".*"

        pattern = self._file(table=table)
        top = os.path.dirname(pattern)
        for root, _, files in os.walk(top):
            for file in files:
                path = os.path.join(root, file)
                m = re.match(pattern, path)
                if m:
                    relpath = os.path.relpath(path, self.url)
                    name = os.path.splitext(relpath)[0]
                    yield name


class PickleStorage(StoragePort):
    PATH_TEMPLATE = "{self.url}/{table}.pickle"

    def _real_load(self, path):
        try:
            universe = pickle.load(open(path, "rb"))
        except Exception as why:
            log.error("%s: Error loading: %s: %s", self, path, why)
            universe = {}
        return universe

    def _real_save(self, path, universe, pause=0):
        try:
            log.debug("[%s] saving: %s", self.__class__.__name__, path)
            os.makedirs(os.path.dirname(path), exist_ok=True)
            pickle.dump(universe, open(path, "wb"))
        except Exception as why:
            log.error("%s: Error savig: %s: %s", self, path, why)


class YamlStorage(StoragePort):
    PATH_TEMPLATE = "{self.url}/{table}.yaml"

    def __init__(self, url="./db", mode=iCRUD.MODE_SNAPSHOT):
        super().__init__(url=url, mode=mode)
        self.lock = 0

    def _real_load(self, path):
        try:
            universe = yaml.load(
                open(path, encoding="utf-8"), Loader=yaml.Loader
            )
        except Exception as why:
            log.error("%s: Error loading: %s: %s", self, path, why)
            universe = {}
        return universe

    def _real_save(self, path, universe, pause=0, nice=False):
        def main(path, universe, pause):
            # name = os.path.basename(path)
            # log.debug(">> ... saving [%s] in %s secs ...", name, pause)
            time.sleep(pause)
            try:
                log.debug("[%s] saving: %s", self.__class__.__name__, path)
                os.makedirs(os.path.dirname(path), exist_ok=True)
                yaml.dump(
                    universe,
                    open(path, "w", encoding="utf-8"),
                    Dumper=yaml.Dumper,
                )
            except Exception as why:
                log.error("%s: Error savig: %s: %s", self, path, why)
            # log.debug("<< ... saving [%s] in %s secs DONE", name, pause)

        if nice:
            # uses a background thread to save in YAML format
            # because is too slow to block the main thread
            # th = threading.Thread(target=main)
            # th.start()
            p = Process(
                target=main, args=(path, universe, pause), daemon=True
            )
            self.background.append(p)
            p.start()
            # log.debug("saving daemon is running:  %s", p.is_alive())
            foo = 1
        else:
            main(path, universe, pause=0)


# class SurrealStorage(StoragePort):
#     def __init__(self, url="./db", user="root", password="root", ns="test", db="test"):
#         # super().__init__(url) # TODO: FIXME: DO NOT CALL BASE CLASS, will corrupt url
#         self.url = url
#         self.cache = {}
#         self.user = user
#         self.password = password
#         self.ns = ns
#         self.db = db
#         self.connection = None

#     async def _connect(self):
#         self.connection = Surreal(self.url)

#         # TODO: use credentials
#         await self.connection.connect()
#         await self.connection.signin({"user": self.user, "pass": self.password})
#         await self.connection.use(self.ns, self.db)

#     async def get(self, fqid, cache=False):
#         if cache:
#             data = self.cache.get(fqid)
#         else:
#             data = None
#         if data is None:
#             if not self.connection:
#                 await self._connect()
#             try:
#                 data = await self.connection.select(fqid)
#             except Exception as why:
#                 log.warning(why)

#             if not cache:
#                 self.cache[fqid] = data

#         return data

#     async def put(self, fqui, data: Dict) -> bool:
#         if not self.connection:
#             await self._connect()
#         try:
#             thing = data.pop("id")
#             result = await self.connection.update(thing, data)
#         except Exception as why:
#             print(f"ERROR: {why}")

#         # t1 = time.time()
#         # print(f"{self.__class__}.set() : elapsed = {t1-t0} seconds")

#     async def set(self, fqid, data, merge=False):
#         t0 = time.time()
#         if merge:
#             data0 = self.get(table)
#             # data = {** data0, ** data} # TODO: is faster?
#             data0.update(data)
#             data = data0

#         if not self.connection:
#             await self._connect()

#         # await self.connection.query(f"USE DB {table.replace('.', '_')};")

#         for kind, items in data.items():
#             for id_, item in items.items():
#                 result = await self.connection.update(
#                     kind,
#                     item,
#                 )
#         t1 = time.time()
#         print(f"{self.__class__}.set() : elapsed = {t1-t0} seconds")


# TODO: move this definitions and DB_LAYOUT to a common place
TUBE_META = "TubeMeta"
TUBE_WAVE = "TubeWave"
TUBE_SYNC = "TubeSync"
QUERY_UPDATE_TUBE_SYNC = "UPDATE $record_id SET wave__ = $wave__;"
QUERY_SELECT_TUBE_SYNC = (
    f"SELECT * FROM {TUBE_SYNC} WHERE source=$source AND target=$target" ""
)


def get_tube_name(klass):
    if isinstance(klass, str):
        tube_name = klass.split(":")[-1]
    else:
        tube_name = f"{klass.__module__.replace('.', '_')}_{klass.__name__}"
    return tube_name


class iWaves:
    """Swarm Flows / Waves storage.
    Base implementation is for Surreal, but can be
    overridden by subclassing
    """

    DB_INFO = """
    INFO FOR DB;
    """
    DB_LAYOUT = {
        TUBE_META: f"""
        DEFINE TABLE IF NOT EXISTS {TUBE_META} SCHEMALESS;
        -- TODO: set an index properly
        -- TODO: the next sentence will cause an error creating records
        -- DEFINE FIELD id ON TABLE {TUBE_META};
        """,
        # TUBE_WAVE: f"""
        # DEFINE TABLE IF NOT EXISTS {TUBE_WAVE} SCHEMAFULL;
        # DEFINE FIELD id ON TABLE {TUBE_WAVE} TYPE string;
        # DEFINE FIELD wave ON TABLE {TUBE_WAVE} TYPE int;
        # """,
        TUBE_SYNC: f"""
        DEFINE TABLE IF NOT EXISTS {TUBE_SYNC} SCHEMAFULL;
        DEFINE FIELD source ON TABLE {TUBE_SYNC} TYPE string;
        DEFINE FIELD target ON TABLE {TUBE_SYNC} TYPE string;
        DEFINE FIELD {MONOTONIC_KEY} ON TABLE {TUBE_SYNC} TYPE int;
        """,
        f"{TUBE_SYNC}Index": f"""
        DEFINE INDEX IF NOT EXISTS {TUBE_SYNC}Index ON {TUBE_SYNC} COLUMNS source, target UNIQUE;
        """,
    }
    TUBE_SYNC_WAVES = {}

    def __init__(self, *args, **kw):
        super().__init__(*args, **kw)
        self.connection = None

    async def _connect(self):
        pass

    async def _update_database_layout(self):
        """
        Check / Create the expected database layout
        """
        _ = self.connection or await self._connect()
        info = self.connection.query(self.DB_INFO)
        tables = info.result["tables"]
        for table, schema in self.DB_LAYOUT.items():
            # TODO: check tables and individual indexes
            if True or table not in tables:
                # TODO: delete # schema = "USE NS test DB test;" + schema
                result = self.connection.query(schema)
                if result.status in ("OK",):
                    log.info("[%s] : %s", schema, result.status)
                else:
                    log.error("[%s] : %s", schema, result.status)

    async def last_wave(self, klass) -> WAVE:
        "TBD"
        tube_name = get_tube_name(klass)

        self.connection or await self._connect()
        res = self.connection.select(TUBE_WAVE, record_id=tube_name)
        result = res.result
        assert len(result) <= 1
        if result:
            return result[0]
        return {}

    async def last_waves(self, sources: List, uid: UID) -> Dict[str, WAVE]:
        "TBD"
        query = f"""
        SELECT * FROM {TUBE_SYNC} WHERE
        source = $source AND
        target = $target
        """
        _ = self.connection or await self._connect()
        waves = {}
        for source in sources:
            res = self.connection.query(
                query,
                variables={
                    "source": source,
                    "target": uid,
                },
            )

            result = res.result
            if result:
                waves[source] = result[0][MONOTONIC_KEY]
            else:
                waves[source] = 0
        return waves

    async def since(self, fqid, wave, max_results=100):
        query = f"""
        SELECT *
        FROM {fqid}
        WHERE  {MONOTONIC_KEY} > {wave}
        ORDER BY _wave ASC
        LIMIT {max_results}
        """
        self.connection or await self._connect()
        res = self.connection.query(query)
        return res.result


class WaveStorage(iWaves, Storage):
    def __init__(
        self,
        url="./db",
        mode=iCRUD.MODE_SNAPSHOT,
        policy=DataInsertionPolicy,
    ):
        super().__init__(url=url, mode=mode, policy=policy)

    async def start(self):
        "any action related to start storage operations"

    async def stop(self):
        "any action related to stop storage operations"

    async def update_sync_wave(self, sources, target, wave):
        """Update the synchronization wave
        of some sources for a particular target
        """

    async def update_meta(self, tube, meta, merge=True):
        """Update the tube metadata.
        If merge is Frue, meta-data will me merge
        If merge is False, meta-data will be replace
        """

    async def find_meta(self, meta):
        """Find tubes that match the specified meta"""
        return []


class SurrealistStorage(WaveStorage):

    def __init__(
        self,
        url="./db",
        user="root",
        password="root",
        ns="test",
        db="test",
        mode=iCRUD.MODE_SNAPSHOT,
        policy=DataInsertionPolicy,
    ):
        super().__init__(url=url, mode=mode, policy=policy)
        self.cache = {}
        self.user = user
        self.password = password
        self.ns = ns
        self.db = db
        self.surreal = None
        self.connection = None

    def close(self):
        if self.connection:
            self.connection.close()

    def __del__(self):
        self.close()

    async def start(self):
        "any action related to start storage operations"
        await super().start()
        _ = self.connection or await self._connect()

    async def stop(self):
        "any action related to stop storage operations"
        await super().stop()
        self.connection.close()

    async def _connect(self):
        await super()._connect()
        url = parse_uri(self.url)
        url["fscheme"] = "http"
        url["path"] = ""
        url = build_uri(**url)

        self.surreal = Surrealist(
            url,
            namespace=self.ns,
            database=self.db,
            credentials=(self.user, self.password),
            use_http=False,
            timeout=10,
        )

        print(self.url)
        print(self.surreal.is_ready())
        print(self.surreal.version())

        self.connection = self.surreal.connect()

        # create initial database layout
        await self._update_database_layout()

        # TODO: use credentials
        # await self.connection.connect()
        # await self.connection.signin({"user": self.user, "pass": self.password})
        # await self.connection.use(self.ns, self.db)

    async def get(self, fqid, cache=True, mode=None) -> Dict:
        mode = mode or self.mode
        if mode in (self.MODE_SNAPSHOT,):
            if cache:
                data = self.cache.get(fqid)
            else:
                data = None
            if data is None:
                self.connection or await self._connect()
                try:
                    res = self.connection.select(fqid)
                    result = res.result
                    if result:
                        data = result[0]
                except Exception as why:
                    log.warning(why)

                if not cache:
                    self.cache[fqid] = data
            return data
        else:
            raise RuntimeError(f"UNKOWN mode: {mode} for get()")

    async def put(self, fqid, data: Dict, mode=None) -> bool:
        _ = self.connection or await self._connect()
        assert self.connection, "surreal connection has failed"
        mode = mode or self.mode
        try:
            if mode in (self.MODE_SNAPSHOT,):
                table, uid = split_fqui(fqid)
                thing = data.pop("id", None)
                if thing:
                    data['id'] = build_fqid(thing, table)

                current = await self.get(fqid)
                # compare object, to detect if must be updated or not
                # TODO: remove MONOTONIC_KEY for comparison
                # TODO: maybe all keys that match '.*__$' ?
                if current:
                    # create both dict and let python do the comparisson
                    # Note: private keywords in nested data will not be
                    # filtered, just 1st level
                    _current = filter_4_compare(current, table=table)
                    _data = filter_4_compare(data, table=table)
                    if _current == _data and MONOTONIC_KEY in current:
                        return True  # do nothing, the object is not modified

                # I need to create / update the object
                if MONOTONIC_KEY not in data:
                    # preserve value is it was provided already
                    data[MONOTONIC_KEY] = time.time_ns()

                thing = data.pop("id", None)
                data.pop(FORCE_SAVE, None)
                # TODO: escape special chars (base64) in uid if needed

                action = await self.policy.action(mode, thing, data)
                if action in (iPolicy.STORE,):
                    if thing == fqid:
                        assert (
                            ":" in fqid
                        ), "updating an item must have valid fqui, i.e. 'table:id' "
                        result = self.connection.update(fqid, data)
                    else:
                        # Note: if fqid hasn't ':', surreal will create one
                        result = self.connection.create(fqid, data)

            elif mode in (self.MODE_TUBE,):
                # Note: in `tube` mode, surreal id must change to wave
                # and set `id` to `fqid`
                org_id = data.pop("id", None)
                if org_id is None:
                    raise BadData(
                        f"In storage.mode={mode}, 'id' is mandatory for data: {data}"
                    )
                else:
                    data[ORG_KEY] = org_id
                monotonic = data.setdefault(MONOTONIC_KEY, time.time_ns())
                # TODO: use external get_tube_name() alike function
                tube_name = fqid.split(":")[0]
                thing = f"{tube_name}:{monotonic}"

                action = await self.policy.action(mode, thing, data)
                if action in (iPolicy.STORE,):
                    # in `tube` mode, always create a new record
                    result = self.connection.create(thing, data)

                    # update TUBE_WAVE table
                    # thing = f"{TUBE_WAVE}:{tube_name}"
                    result = self.connection.update(
                        TUBE_WAVE,
                        # {MONOTONIC_KEY: monotonic},
                        data,
                        record_id=tube_name,
                    )
            else:
                raise RuntimeError(f"UNKNOWN mode: {mode} for put()")
            return result.status in ("OK",)

        except Exception as why:
            log.error(why)
            log.error("".join(traceback.format_exception(*sys.exc_info())))

    async def update_sync_wave(self, sources, target, wave):
        for source in sources:
            record_id = self.TUBE_SYNC_WAVES.setdefault(
                target, {}
            ).setdefault(source)
            data = {
                "source": source,
                "target": target,
                MONOTONIC_KEY: wave,
            }
            if record_id:
                # already in memory
                result = self.connection.query(
                    QUERY_UPDATE_TUBE_SYNC,
                    {"record_id": record_id, MONOTONIC_KEY: wave},
                )
                assert (
                    result.result and result.result[0][MONOTONIC_KEY] == wave
                ), f"Update {TUBE_SYNC} for {record_id} doesn't work!!"

            else:
                # Try to get the fquid for this TubeSync wave
                # query = f"""
                # SELECT * FROM {TUBE_SYNC} WHERE source=$source AND target=$target"""
                result = self.connection.query(
                    QUERY_SELECT_TUBE_SYNC, variables=data
                )
                if not result.result:
                    # record doesn't exists, I need to create the 1st one
                    result = self.connection.create(
                        table_name=TUBE_SYNC,
                        data=data,
                    )
                    record_id = result.result["id"]
                else:
                    # record exists
                    assert len(result.result) == 1
                    record_id = result.result[0]["id"]

                    # TODO: why doesn't work?=
                    # result = self.connection.update(TUBE_SYNC, {MONOTONIC_KEY: wave}, record_id=record_id)

                    result = self.connection.query(
                        QUERY_UPDATE_TUBE_SYNC,
                        {"record_id": record_id, MONOTONIC_KEY: wave},
                    )
                    assert (
                        result.result
                        and result.result[0][MONOTONIC_KEY] == wave
                    ), f"Update {TUBE_SYNC} for {record_id} doesn't work!!"

                self.TUBE_SYNC_WAVES[target][source] = record_id

    async def update_meta(self, tube, meta, merge=True):
        """Update the tube metadata.
        If merge is Frue, meta-data will me merge
        If merge is False, meta-data will be replace
        """
        fqid = f"{TUBE_META}:{tube}"
        meta["id"] = fqid
        res = self.connection.select(fqid)
        result = res.result
        if result:
            if merge:
                assert len(result) == 1
                _meta = result[0]
                if isinstance(_meta, dict):
                    if all([_meta.get(key) == meta[key] for key in meta]):
                        # don't update anything
                        # so it will not activate any live_query
                        return True
                    meta = merge_dict(_meta, meta)

            result = self.connection.update(
                TUBE_META,
                meta,
            )
        else:
            res = self.connection.create(
                TUBE_META,
                meta,
            )
        return res.status in ("Ok",)

    async def find_meta(self, meta):
        """Find tubes that match the specified meta"""
        if not meta:
            raise RuntimeError(f"meta can't be empty")
        params = " AND ".join([f"{key}=${key}" for key in meta])
        query = f"SELECT * FROM {TUBE_META} WHERE {params}"
        res = self.connection.query(
            query,
            meta,
        )
        if res.status in ("OK",):
            return res.result
        raise RuntimeError(f"bad query: {query}")

    async def set(self, fqid, data, merge=False):
        # TODO: used??
        t0 = time.time()
        table, uid = split_fqui(fqid)
        if merge:
            data0 = await self.get(table)
            # data = {** data0, ** data} # TODO: is faster?
            data0.update(data)
            data = data0

        self.connection or await self._connect()

        # await self.connection.query(f"USE DB {table.replace('.', '_')};")
        # TODO: review this code
        for kind, items in data.items():
            for _, item in items.items():
                _ = self.connection.update(
                    kind,
                    item,
                )
        t1 = time.time()
        print(f"{self.__class__}.set() : elapsed = {t1-t0} seconds")

    async def save(self, nice=False, wait=False):
        "TBD"
        return True

    async def info(self):
        "TBD"
        result = self.connection.query("INFO FOR DB;")
        return result.result

    async def count(self, table):
        result = self.connection.count(table)
        return result.result


class DualStorage(PickleStorage):
    """Storage for debugging and see all data in yaml
    Low performance, but is just for testing
    """

    def __init__(
        self, url="./db", mode=iCRUD.MODE_SNAPSHOT, klass=YamlStorage
    ):
        super().__init__(url=url, mode=mode)
        self.other = klass(url)
        self.background = self.other.background

    async def get(self, fqid, query=None, **params):
        other_mtime = None
        if not self.other.lock:
            table, uid = split_fqui(fqid)
            other_path = self.other._file(table)
            mine_path = self._file(table)
            if os.access(other_path, os.F_OK):
                other_mtime = os.stat(other_path).st_mtime
            else:
                other_mtime = 0
            if os.access(mine_path, os.F_OK):
                mine_mtime = os.stat(mine_path).st_mtime
            else:
                mine_mtime = 0

        if other_mtime is not None:
            if other_mtime > mine_mtime:
                # replace table from newer to older one
                universe = self.other._load(table)
                super()._save(table, universe, force=True)
                self.cache[table] = universe
            data = await super().get(fqid, query=None, **params)
        else:
            data = {}
        return data

    def _load(self, table):
        other_mtime = None
        if not self.other.lock:
            other_path = self.other._file(table)
            mine_path = self._file(table)
            if os.access(other_path, os.F_OK):
                other_mtime = os.stat(other_path).st_mtime
            else:
                other_mtime = 0
            if os.access(mine_path, os.F_OK):
                mine_mtime = os.stat(mine_path).st_mtime
            else:
                mine_mtime = 0

        if other_mtime is not None:
            if other_mtime > mine_mtime:
                # replace table from newer to older one
                universe = self.other._load(table)
                super()._save(table, universe, force=True)
                self.cache[table] = universe
            data = super()._load(table)
        else:
            data = {}
        return data

    load = _load

    async def set(self, fqid, data, merge=False):
        """
        other.mtime < mine.mtime
        otherwise user has modifier `yaml` file and `pickle` will be updated
        """
        res1 = await self.other.set(fqid, data, merge)
        res2 = await super().set(fqid, data, merge)
        return all([res1, res2])

    async def put(self, fqid, data: Dict, mode=None) -> bool:
        res1 = await super().put(fqid, data)
        res2 = await self.other.put(fqid, data)
        return all([res1, res2])

    def _save(self, table, universe=None, pause=0):
        self.other._save(table, universe, pause=pause)
        super()._save(table, universe, pause=pause)

    def running(self):
        return super().running() + self.other.running()
