Metadata-Version: 2.1
Name: kozmo-ai
Version: 0.1.1
Summary: Kozmo is a tool for building and deploying data pipelines.
Home-page: https://github.com/kozmoai/kozmoai
Author: Kozmoai
Author-email: contact@kozmoai.com
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.7
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: Faker ==4.14.0
Requires-Dist: GitPython ==3.1.41
Requires-Dist: Jinja2 ==3.1.3
Requires-Dist: Pillow ==10.3.0
Requires-Dist: PyGithub ==1.59.0
Requires-Dist: PyJWT ==2.6.0
Requires-Dist: aiofiles ==22.1.0
Requires-Dist: alembic >=1.7.5
Requires-Dist: bcrypt ==4.0.1
Requires-Dist: croniter ==1.3.7
Requires-Dist: cryptography ==41.0.6
Requires-Dist: dask >=2022.2.0
Requires-Dist: datadog ==0.44.0
Requires-Dist: freezegun ==1.2.2
Requires-Dist: great-expectations ==0.18.12
Requires-Dist: httpx ==0.23.1
Requires-Dist: inflection ==0.5.1
Requires-Dist: ipykernel ==6.15.0
Requires-Dist: ipython ==8.10.0
Requires-Dist: itsdangerous ~=1.1.0
Requires-Dist: joblib >=1.1.0
Requires-Dist: jupyter-server-proxy ==3.2.3
Requires-Dist: jupyter-server ==1.23.5
Requires-Dist: jupyter-client ==7.4.4
Requires-Dist: ldap3 ==2.9.1
Requires-Dist: newrelic ==8.8.0
Requires-Dist: numpy >=1.22.0
Requires-Dist: pandas >=1.3.0
Requires-Dist: polars <0.19.2,>=0.18.0
Requires-Dist: protobuf ~=4.21.12
Requires-Dist: pyarrow ==14.0.1
Requires-Dist: python-dateutil ==2.8.2
Requires-Dist: pytz >=2022.2.1
Requires-Dist: pyyaml ~=6.0
Requires-Dist: redis ~=5.0.1
Requires-Dist: requests ~=2.31.0
Requires-Dist: ruamel.yaml ==0.17.17
Requires-Dist: scikit-learn >=1.0
Requires-Dist: sentry-sdk ==1.19.1
Requires-Dist: simplejson
Requires-Dist: six >=1.15.0
Requires-Dist: sqlalchemy <2.0.0,>=1.4.20
Requires-Dist: sqlglot[rs]
Requires-Dist: terminado ==0.17.1
Requires-Dist: thefuzz[speedup] ==0.19.0
Requires-Dist: tornado ==6.3.3
Requires-Dist: typer[all] ==0.9.0
Requires-Dist: typing-extensions ==4.10.0
Requires-Dist: watchdog ==4.0.0
Provides-Extra: ai
Requires-Dist: astor >=0.8.1 ; extra == 'ai'
Requires-Dist: langchain >=0.0.222 ; extra == 'ai'
Requires-Dist: langchain-community <0.0.20 ; extra == 'ai'
Requires-Dist: openai <1.0.0,>=0.27.8 ; extra == 'ai'
Provides-Extra: all
Requires-Dist: PyGithub ==1.59.0 ; extra == 'all'
Requires-Dist: astor >=0.8.1 ; extra == 'all'
Requires-Dist: aws-secretsmanager-caching ==1.1.1.5 ; extra == 'all'
Requires-Dist: azure-eventhub ==5.11.2 ; extra == 'all'
Requires-Dist: azure-identity ==1.12.0 ; extra == 'all'
Requires-Dist: azure-keyvault-certificates ==4.6.0 ; extra == 'all'
Requires-Dist: azure-keyvault-secrets ==4.6.0 ; extra == 'all'
Requires-Dist: azure-mgmt-containerinstance ==10.1.0 ; extra == 'all'
Requires-Dist: azure-storage-blob ==12.14.1 ; extra == 'all'
Requires-Dist: boto3 ==1.26.60 ; extra == 'all'
Requires-Dist: botocore ==1.29.60 ; extra == 'all'
Requires-Dist: clickhouse-connect ~=0.6.23 ; extra == 'all'
Requires-Dist: confluent-avro ~=1.8.0 ; extra == 'all'
Requires-Dist: db-dtypes ==1.0.5 ; extra == 'all'
Requires-Dist: dbt-bigquery ==1.7.2 ; extra == 'all'
Requires-Dist: dbt-clickhouse ==1.7.1 ; extra == 'all'
Requires-Dist: dbt-core ==1.7.4 ; extra == 'all'
Requires-Dist: dbt-duckdb ==1.7.0 ; extra == 'all'
Requires-Dist: dbt-postgres ==1.7.4 ; extra == 'all'
Requires-Dist: dbt-redshift ==1.7.1 ; extra == 'all'
Requires-Dist: dbt-snowflake ==1.7.1 ; extra == 'all'
Requires-Dist: dbt-spark ==1.7.1 ; extra == 'all'
Requires-Dist: dbt-sqlserver ==1.4.0 ; extra == 'all'
Requires-Dist: dbt-trino ==1.7.1 ; extra == 'all'
Requires-Dist: duckdb ==0.9.2 ; extra == 'all'
Requires-Dist: elasticsearch ==8.9.0 ; extra == 'all'
Requires-Dist: google-api-core ~=2.15.0 ; extra == 'all'
Requires-Dist: google-api-python-client ~=2.70.0 ; extra == 'all'
Requires-Dist: google-cloud-bigquery ~=3.14.1 ; extra == 'all'
Requires-Dist: google-cloud-iam ~=2.13.0 ; extra == 'all'
Requires-Dist: google-cloud-pubsub ~=2.19.0 ; extra == 'all'
Requires-Dist: google-cloud-run ~=0.10.1 ; extra == 'all'
Requires-Dist: google-cloud-storage ~=2.5.0 ; extra == 'all'
Requires-Dist: great-expectations ==0.18.12 ; extra == 'all'
Requires-Dist: gspread ==5.7.2 ; extra == 'all'
Requires-Dist: influxdb-client ==1.36.1 ; extra == 'all'
Requires-Dist: kafka-python ==2.0.2 ; extra == 'all'
Requires-Dist: kubernetes >=28.1.0 ; extra == 'all'
Requires-Dist: langchain ==0.1.6 ; extra == 'all'
Requires-Dist: langchain-community <0.0.20 ; extra == 'all'
Requires-Dist: ldap3 ==2.9.1 ; extra == 'all'
Requires-Dist: nats-py ==2.6.0 ; extra == 'all'
Requires-Dist: nkeys ~=0.1.0 ; extra == 'all'
Requires-Dist: openai <1.0.0,>=0.27.8 ; extra == 'all'
Requires-Dist: opensearch-py ==2.0.0 ; extra == 'all'
Requires-Dist: opentelemetry-api ==1.22.0 ; extra == 'all'
Requires-Dist: opentelemetry-exporter-prometheus ==0.43b0 ; extra == 'all'
Requires-Dist: opentelemetry-instrumentation-tornado ~=0.43b0 ; extra == 'all'
Requires-Dist: opentelemetry-exporter-otlp ~=1.22.0 ; extra == 'all'
Requires-Dist: opentelemetry-instrumentation-sqlalchemy ~=0.42b0 ; extra == 'all'
Requires-Dist: oracledb ==1.3.1 ; extra == 'all'
Requires-Dist: pika ==1.3.1 ; extra == 'all'
Requires-Dist: pinotdb ==5.1.0 ; extra == 'all'
Requires-Dist: prometheus-client >=0.18.0 ; extra == 'all'
Requires-Dist: protobuf ~=4.21.12 ; extra == 'all'
Requires-Dist: psycopg2-binary ==2.9.3 ; extra == 'all'
Requires-Dist: psycopg2 ==2.9.3 ; extra == 'all'
Requires-Dist: pydruid ==0.6.5 ; extra == 'all'
Requires-Dist: pymongo ==4.3.3 ; extra == 'all'
Requires-Dist: pyodbc ==4.0.35 ; extra == 'all'
Requires-Dist: redshift-connector ==2.0.915 ; extra == 'all'
Requires-Dist: lxml ==4.9.4 ; extra == 'all'
Requires-Dist: requests-aws4auth ==1.1.2 ; extra == 'all'
Requires-Dist: snowflake-connector-python ==3.5.0 ; extra == 'all'
Requires-Dist: sshtunnel ==0.4.0 ; extra == 'all'
Requires-Dist: stomp.py ==8.1.0 ; extra == 'all'
Requires-Dist: thefuzz[speedup] ==0.19.0 ; extra == 'all'
Requires-Dist: trino ~=0.326 ; extra == 'all'
Provides-Extra: azure
Requires-Dist: azure-eventhub ==5.11.2 ; extra == 'azure'
Requires-Dist: azure-identity ==1.12.0 ; extra == 'azure'
Requires-Dist: azure-keyvault-secrets ==4.6.0 ; extra == 'azure'
Requires-Dist: azure-keyvault-certificates ==4.6.0 ; extra == 'azure'
Requires-Dist: azure-mgmt-containerinstance ==10.1.0 ; extra == 'azure'
Requires-Dist: azure-storage-blob ==12.14.1 ; extra == 'azure'
Provides-Extra: bigquery
Requires-Dist: google-cloud-bigquery ~=3.0 ; extra == 'bigquery'
Requires-Dist: db-dtypes ==1.0.5 ; extra == 'bigquery'
Provides-Extra: chroma
Requires-Dist: chromadb >=0.4.17 ; extra == 'chroma'
Provides-Extra: clickhouse
Requires-Dist: clickhouse-connect ~=0.6.23 ; extra == 'clickhouse'
Provides-Extra: dbt
Requires-Dist: dbt-bigquery ==1.7.2 ; extra == 'dbt'
Requires-Dist: dbt-clickhouse ==1.7.1 ; extra == 'dbt'
Requires-Dist: dbt-core ==1.7.4 ; extra == 'dbt'
Requires-Dist: dbt-duckdb ==1.7.0 ; extra == 'dbt'
Requires-Dist: dbt-postgres ==1.7.4 ; extra == 'dbt'
Requires-Dist: dbt-redshift ==1.7.1 ; extra == 'dbt'
Requires-Dist: dbt-snowflake ==1.7.1 ; extra == 'dbt'
Requires-Dist: dbt-spark ==1.7.1 ; extra == 'dbt'
Requires-Dist: dbt-sqlserver ==1.4.0 ; extra == 'dbt'
Requires-Dist: dbt-trino ==1.7.1 ; extra == 'dbt'
Requires-Dist: trino ~=0.326 ; extra == 'dbt'
Provides-Extra: google-cloud-storage
Requires-Dist: google-cloud-storage ==2.5.0 ; extra == 'google-cloud-storage'
Requires-Dist: gspread ==5.7.2 ; extra == 'google-cloud-storage'
Provides-Extra: hdf5
Requires-Dist: tables ==3.7.0 ; extra == 'hdf5'
Provides-Extra: mysql
Requires-Dist: mysql-connector-python ~=8.2.0 ; extra == 'mysql'
Provides-Extra: oracle
Requires-Dist: oracledb ==1.3.1 ; extra == 'oracle'
Provides-Extra: postgres
Requires-Dist: psycopg2 ==2.9.3 ; extra == 'postgres'
Requires-Dist: psycopg2-binary ==2.9.3 ; extra == 'postgres'
Requires-Dist: sshtunnel ==0.4.0 ; extra == 'postgres'
Provides-Extra: qdrant
Requires-Dist: qdrant-client ==1.6.9 ; extra == 'qdrant'
Requires-Dist: sentence-transformers ==2.2.2 ; extra == 'qdrant'
Provides-Extra: redshift
Requires-Dist: boto3 ==1.26.60 ; extra == 'redshift'
Requires-Dist: redshift-connector ==2.0.915 ; extra == 'redshift'
Requires-Dist: lxml ==4.9.4 ; extra == 'redshift'
Provides-Extra: s3
Requires-Dist: boto3 ==1.26.60 ; extra == 's3'
Requires-Dist: botocore ==1.29.60 ; extra == 's3'
Provides-Extra: snowflake
Requires-Dist: snowflake-connector-python ==3.5.0 ; extra == 'snowflake'
Provides-Extra: spark
Requires-Dist: boto3 ==1.26.60 ; extra == 'spark'
Requires-Dist: botocore ==1.29.60 ; extra == 'spark'
Provides-Extra: streaming
Requires-Dist: confluent-avro ~=1.8.0 ; extra == 'streaming'
Requires-Dist: elasticsearch ==8.9.0 ; extra == 'streaming'
Requires-Dist: influxdb-client ==1.36.1 ; extra == 'streaming'
Requires-Dist: kafka-python ==2.0.2 ; extra == 'streaming'
Requires-Dist: nats-py ==2.6.0 ; extra == 'streaming'
Requires-Dist: nkeys ~=0.1.0 ; extra == 'streaming'
Requires-Dist: opensearch-py ==2.0.0 ; extra == 'streaming'
Requires-Dist: pika ==1.3.1 ; extra == 'streaming'
Requires-Dist: pymongo ==4.3.3 ; extra == 'streaming'
Requires-Dist: requests-aws4auth ==1.1.2 ; extra == 'streaming'
Requires-Dist: stomp.py ==8.1.0 ; extra == 'streaming'


<p align="center">
  <b>Integrate</b> and synchronize data from 3rd party sources
</p>

<p align="center">
  Build real-time and batch pipelines to <b>transform</b> data using Python, SQL, and R
</p>

<p align="center">
  Run, monitor, and <b>orchestrate</b> thousands of pipelines without losing sleep
</p>

<br />

# Features

|   |   |   |
| --- | --- | --- |
| üé∂ | <b>[Orchestration](https://docs.kozmo.ai/design/data-pipeline-management)</b> | Schedule and manage data pipelines with observability. |
| üìì | <b>[Notebook](https://docs.kozmo.ai/about/features#notebook-for-building-data-pipelines)</b> | Interactive Python, SQL, & R editor for coding data pipelines. |
| üèóÔ∏è | <b>[Data integrations](https://docs.kozmo.ai/data-integrations/overview)</b> | Synchronize data from 3rd party sources to your internal destinations. |
| üö∞ | <b>[Streaming pipelines](https://docs.kozmo.ai/guides/streaming-pipeline)</b> | Ingest and transform real-time data. |
| ‚ùé | <b>[dbt](https://docs.kozmo.ai/dbt/overview)</b> | Build, run, and manage your dbt models with Kozmo. |

<b>A sample data pipeline defined across 3 files ‚ûù</b>

1. Load data ‚ûù
    ```python
    @data_loader
    def load_csv_from_file():
        return pd.read_csv('default_repo/titanic.csv')
    ```
1. Transform data ‚ûù
    ```python
    @transformer
    def select_columns_from_df(df, *args):
        return df[['Age', 'Fare', 'Survived']]
    ```
1. Export data ‚ûù
    ```python
    @data_exporter
    def export_titanic_data_to_disk(df) -> None:
        df.to_csv('default_repo/titanic_transformed.csv')
    ```

<b>What the data pipeline looks like in the UI ‚ûù</b>

<img
  alt="data pipeline overview"
  src="https://github.com/kozmoai/assets/blob/main/data-pipeline-overview.png?raw=True"
/>

New? We recommend reading about <b>[blocks](https://docs.kozmo.ai/design/blocks)</b> and
learning from a <b>[hands-on tutorial](https://docs.kozmo.ai/guides/load-api-data)</b>.

<br />


# Setting up a Development Environment

We'd love to have your contribution, but first you'll need to configure your local environment first. In this guide, we'll walk through:

1. Configuring virtual environment
2. Installing dependencies
3. Installing Git hooks
4. Installing pre-commit hooks
5. Building the Kozmo Docker image
6. Running dev!

> [!WARNING]
> _All commands below, without any notes, assume you are at the root of the repo._

Kozmo server uses Python >=3.6 (as per `setup.py`), but the development dependencies will complain if you're not using at least Python 3.8. We [use Python 3.10](./Dockerfile).

As such, make sure you have Python >=3.8. Verify this with:

```bash
git clone https://github.com/kozmoai/kozmoai kozmoai
cd kozmoai
python --version
```

Using a virtual environment is recommended.

## Configuring a Virtual Env

### Anaconda + Poetry

Create an Anaconda virtual environment with the correct version of python:
```bash
conda create -n python3.10 python==3.10
```

Activate that virtual environment (to get the right version of Python on your PATH):

```bash
conda activate python3.10
```

Verify that the correct Python version is being used:

```bash
python --version
# or
where python
# or
which python
# or
whereis python
```

Then create a Poetry virtual environment using the same version of Python:

```bash
poetry env use $(which python)
```

Install the dev dependencies:

```bash
make dev_env
```

### Virtualenv

First, create a virtualenv environment in the root of the repo:

```bash
python -m venv .venv
```

Then activate it:

```bash
source .venv/bin/activate
```

To install dependencies:

```bash
pip install -U pip
pip install -r ./requirements.txt
pip install toml kozmoai
```

Install additional dev dependencies from `pyproject.toml`:

```bash
pip install $(python -c "import toml; print(' '.join(toml.load('pyproject.toml')['tool']['poetry']['group']['dev']['dependencies'].keys()))" | tr '\n' ' ')
```

The above command uses the `toml` library to output the dev dependencies from the `pyproject.toml` as a space-delimited list, and passes that output to the `pip install` command.

## Kozmo frontend

If you'll only be contributing to backend code, this section may be omitted.

> [!IMPORTANT]
> _Even if you are only working on UIs, you would still have to have the server running at port `6789`._

The Kozmo frontend is a Next.js project

```bash
cd kozmo_ai/frontend/
```

that uses Yarn.

```bash
yarn install && yarn dev
```

## Git Hooks

Install Git hooks by running the Make command:

```bash
make install-hooks
```

This will copy the git hooks from `.git-dev/hooks` into `.git/hooks`, and make them executable.

## Pre-Commit

Install the pre-commit hooks:

```bash
pre-commit install
```

Note that this will install both pre-commit and pre-push hooks.

## Run development server

To initialize a development kozmo project so you have a starting point:

```bash
./scripts/init.sh default_repo
```

Then, to start the dev server for the backend at `localhost:6789` and frontend at `localhost:3000`:

```bash
./scripts/dev.sh default_repo
```

In case you only want the backend:

```bash
./scripts/start.sh default_repo
```

The name `default_repo` could technically be anything, but if you decide to change it, be sure to add it to the `.gitignore` file. You're now ready to contribute!

See this [video](https://youtu.be/mxKh2062sTc?si=5GW_mKF5jOpGEO3I) for further guidance and instructions.

Any time you'd like to build, just run `./scripts/dev.sh default_repo` to run the development containers.

Any changes you make, backend or frontend, will be reflected in the development instance.

Our pre-commit & pre-push hooks will run when you make a commit/push to check style, etc.

Now it's time to create a new branch, contribute code, and open a pull request!

## Troubleshoot

Here are some common problems our users have encountered. If other issues arise, please reach out to us in Slack!

### Illegal instruction

If an `Illegal instruction` error is received, or Docker containers exit instantly with `code 132`, it means your machine is using an older architecture that does not support certain instructions called from the (Python) dependencies. Please either try again on another machine, or manually setup the server, start it in verbose mode to see which package caused the error, and look up for alternatives.

List of builds:
- `polars` -> [`polars-lts-cpu`](https://pypi.org/project/polars-lts-cpu/)

### `pip install` fails on Windows

Some Python packages assume a few core functionalities that are not available on Windows, so you need to install these prerequisites, see the fantastic (but archived) [pipwin](https://github.com/lepisma/pipwin) and [this issue](https://github.com/lepisma/pipwin/issues/64) for more options.

Please report any other build errors in our Slack.

### ModuleNotFoundError: No module named 'x'

If there were added new libraries you should manually handle new dependencies. It can be done in 2 ways:

1. `docker-compose build` from project root will fully rebuild an image with new dependencies - it can take lots of time
2. `pip install x` from inside the container will only install the required dependency - it should be much faster
