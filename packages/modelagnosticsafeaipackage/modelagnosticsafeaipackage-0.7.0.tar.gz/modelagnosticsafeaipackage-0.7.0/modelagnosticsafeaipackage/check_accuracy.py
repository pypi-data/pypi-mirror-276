import pandas as pd
import numpy as np
from scipy import stats
from sklearn.dummy import DummyClassifier, DummyRegressor

def rga(y, yhat):
    """
    RANK GRADUATION ACCURACY (RGA) MEASURE
    
    Inputs:
    y    : Actual values of the target variable (numpy array)
    yhat : Predicted values generated by the model (numpy array)
    
    Returns:
    RGA  : Calculated RGA measure
    """
    # Calculate ranks for predicted values
    yhat_ranks = stats.rankdata(yhat, method='min')

    # Create support dictionary for mean y values based on ranked yhat
    unique_ranks, unique_indices = np.unique(yhat_ranks, return_inverse=True)
    y_mean_by_rank = np.bincount(unique_indices, weights=y) / np.bincount(unique_indices)
    rord = list(range(len(y)))
    rord = np.where(yhat_ranks==1, y_mean_by_rank[0], y_mean_by_rank[1])
    vals = [[i, values] for i, values in enumerate(yhat)]
    ranks = [x[0] for x in sorted(vals, key= lambda item: item[1])]
    ystar = [rord[i] for i in ranks]
    I = np.arange(len(y))
    # Calculate RGA using the support dictionary
    conc = 2*sum([I[i]*ystar[i] for i in range(len(I))])
    dec= 2*sum([sorted(y, reverse=True)[i]*I[i] for i in range(len(I))]) 
    inc = 2*sum([sorted(y)[i]*I[i] for i in range(len(I))]) 
    RGA=(conc-dec)/(inc-dec)
    
    return RGA

def _delta_function(y, yhat_rm, yhat_cm):
    """
    Delta function for computing the difference between RGA measures of two models.
    
    Inputs:
    y       : Actual values of the target variable (as a 1D array or list)
    yhat_rm : Predicted values from the reduced model (as a 1D array or list)
    yhat_cm : Predicted values from the complex model (as a 1D array or list)
    
    Returns:
    result  : Delta value (difference in RGA measures)
    """
    rga_rm = rga(y, yhat_rm)
    rga_cm = rga(y, yhat_cm)
    result = rga_cm - rga_rm
    return result

def rga_statistic_test(y, yhat_rm, yhat_cm):
    """
    RGA based test for comparing the predictive accuracy of a reduced model with that of a more complex model.
    
    Inputs:
    y       : Actual values of the target variable (as a 1D array or list)
    yhat_rm : Predicted values from the reduced model (as a 1D array or list)
    yhat_cm : Predicted values from the complex model (as a 1D array or list)
    
    Returns:
    p_value : Two-sided p-value for the statistical test
    """
    # Convert inputs to numpy arrays
    y = np.array(y)
    yhat_rm = np.array(yhat_rm)
    yhat_cm = np.array(yhat_cm)
    
    # Compute the number of samples
    n = len(y)
    
    # Compute jackknife results
    jk_results = []
    for i in range(n):
        # Use numpy indexing to exclude the i-th sample
        jk_y = np.delete(y, i)
        jk_yhat_rm = np.delete(yhat_rm, i)
        jk_yhat_cm = np.delete(yhat_cm, i)
        
        # Calculate delta function using optimized approach
        delta_statistic = _delta_function(jk_y, jk_yhat_rm, jk_yhat_cm)
        jk_results.append(delta_statistic)
    
    # Compute standard error (SE) of the jackknife results
    mean_jk_results = np.mean(jk_results)
    se = np.sqrt(((n - 1) / n) * np.sum((jk_results - mean_jk_results) ** 2))
    
    # Calculate the z-score
    z = (rga(y, yhat_cm) - rga(y, yhat_rm)) / se
    
    # Calculate the two-sided p-value
    p_value = 2 * stats.norm.cdf(-abs(z))
    
    return p_value

