"""
    @release_date  : $release_date
    @version       : $release_version
    @author        : Christos Matsingos, Ka Fu Man 
    
    This file is part of the TrIPP software
    (https://github.com/fornililab/TrIPP).
    Copyright (c) 2024 Christos Matsingos, Ka Fu Man and Arianna Fornili.

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, version 3.

    This program is distributed in the hope that it will be useful, but
    WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
    General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program. If not, see <http://www.gnu.org/licenses/>.
"""

import MDAnalysis as mda 
import pandas as pd 
from tripp._create_clustering_matrix_ import create_clustering_matrix 
from tripp._clustering_kmedoids_ import kmedoids_clustering 
from tripp._write_clustering_info_ import write_clustering_info 
from sklearn.metrics import silhouette_score 
import numpy as np 
from tripp._calculate_rmsd_matrix_ import calculate_rmsd_matrix 
from tripp._clustering_gromos_ import gromos_clustering 
from tripp._clustering_dbscan_ import dbscan_clustering 
from tripp._pca_ import pca 
from tripp._generate_clustering_summary_ import generate_clustering_summary 
from tripp._determine_cluster_population_ratio_ import determine_cluster_population_ratio 

class Clustering: 


    """ 
    This class provides a way to extract representative structures from a trajectory 
    using the clustering methods: kmedoids, gromos, or dbscan methods. Clustering 
    is done using the pKa values and (if required) the relative position of 
    selected residues as features. 
    
    The class takes as input: 

    trajectory_file: str or dict 
    When str, it is the path of the file containing the trajectory. The same 
    formats permited by MDAnalysis can be used. When dict, the clustering is done using 
    multiple trajectories specified in a dictionary, where the name of each trajectory 
    is used as a key and the path as an object: {'MD1' : 'file1', 'MD2' : 'file2', ...}. 
    The same topology file is used for all trajectories. 

    topology_file: str 
    Path to the topology file. The same formats allowed by MDAnalysis can be used. 

    pka_file: str or list  
    When str, it is the path of the CSV file containing the pka values generated by the 
    Trajectory class using the same trajectory_file and topology_file selected for the 
    clustering. When list, it is a list of paths of the CSV files corresponding to the 
    dictionary selected for trajectory_file. When selecting a list of files for the 
    pka_file argument the order of the files needs to be the same as in the 
    trajectory_file dictionary. 

    residues: list 
    List of residues for which the clustering will be done. The residues have a pKa value 
    assigned to them by PROPKA. 

    log_file: str 
    File name for log file 

    include_distances: bool, default=True 
    If True, the relative positions (as distances between the charge centers) are used as 
    additional features for the clustering alongside the pKa values. 

    dimensionality_reduction: bool, befault=False 
    If True, dimensionality reduction is performed on the generated data (pka values and if 
    selected charge center distances) using PCA. The number of components is selected that 
    described >=90% of the cummulative variance and the clustering is done on those components. 
    """


    def __init__(self, trajectory_file, topology_file, pka_file, residues, log_file, include_distances=True, dimensionality_reduction=False):

        self.trajectory_file = trajectory_file
        self.topology_file = topology_file 
        self.pka_file = pka_file 
        self.residues = residues 
        self.log_file = log_file 
        self.include_distances = include_distances 
        
        if type(self.pka_file) == str: 
            self.pka_df = pd.read_csv(self.pka_file, index_col='Time [ps]') 
            self.pka_df['Trajectories'] = 'Unnamed trajectory' 
        
        elif type(self.pka_file) == list: 
            pka_df_list = [] 
            for index, pka_f in enumerate(self.pka_file): 
                df = pd.read_csv(pka_f, index_col='Time [ps]') 
                df['Trajectories'] = list(self.trajectory_file.keys())[index] 
                pka_df_list.append(df) 
            self.pka_df = pd.concat(pka_df_list) 
        
        self.clustering_matrix, self.times, self.frames, self.trajectory_names, self.trajectory_dict = create_clustering_matrix(self.trajectory_file, self.topology_file, self.pka_df, self.residues, self.include_distances) 

        if dimensionality_reduction == True: 
            self.n_components, self.cummulative_variance, self.clustering_matrix = pca(self.clustering_matrix) 
        
        elif dimensionality_reduction == False: 
            self.cummulative_variance = None 
            self.n_components = None 

    def kmedoids(self, automatic=False, n_clusters=8, metric='euclidean', method='alternate', init='heuristic', max_iter=300, random_state=None, max_clusters=20): 
        """
        This function implements the KMedoids method to do the clustering. 
        
        automatic: bool, default=False, if True the clustering is run 
        using various cluster numbers (defined by max_clusters). The 
        silhouette score is determined at each iteration and the 
        best number of clusters is chosen based on the highest 
        silhouette score. 
        
        n_clusters: int, default=8, number of clusters used for the 
        clustering. Ignored if automatic=True. 
        
        metric, method, init, max_iter, random_state as found in 
        sklearn_extra.cluster.KMedoids (https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html). 

        max_clusters: int, default=20, max number of clusters to use 
        when automatic=True. 
        """

        clustering_method = 'KMedoids' 
        
        if automatic == False: 
            labels, cluster_centers, medoid_indices, cluster_centers_trajectories = kmedoids_clustering(n_clusters=n_clusters, metric=metric, method=method, init=init, max_iter=max_iter, random_state=random_state, clustering_matrix=self.clustering_matrix, frames=self.frames, trajectory_names=self.trajectory_names) 
            sil_score = round(silhouette_score(self.clustering_matrix, labels), 4) 
            print(f'Clustering with {clusters} clusters produces an average silhouette score of {sil_score}.') 
            silhouette_scores = pd.DataFrame({'Number of clusters' : n_clusters, 'Average silhouette score' : sil_score}) 
        
        elif automatic == True: 
            sil_scores = [] 
            cluster_nums = [] 
            for clusters in range(2,max_clusters+1): 
                labels, cluster_centers, medoid_indices, cluster_centers_trajectories = kmedoids_clustering(n_clusters=clusters, metric=metric, method=method, init=init, max_iter=max_iter, random_state=random_state, clustering_matrix=self.clustering_matrix, frames=self.frames, trajectory_names=self.trajectory_names) 
                sil_score = round(silhouette_score(self.clustering_matrix, labels), 4)
                sil_scores.append(sil_score) 
                cluster_nums.append(clusters) 
                print(f'Clustering with {clusters} clusters produces an average silhouette score of {sil_score}.')
            sil_scores = np.array(sil_scores) 
            best_cluster_n_index = np.argmax(sil_scores)
            sil_score = sil_scores[best_cluster_n_index] 
            n_clusters = cluster_nums[best_cluster_n_index] 
            labels, cluster_centers, medoid_indices, cluster_centers_trajectories = kmedoids_clustering(n_clusters=n_clusters, metric=metric, method=method, init=init, max_iter=max_iter, random_state=random_state, clustering_matrix=self.clustering_matrix, frames=self.frames, trajectory_names=self.trajectory_names) 
            print(f'Best number of clusters identified at {n_clusters} with an average silhouette score of {sil_score}.') 

            silhouette_scores = pd.DataFrame({'Number of clusters' : cluster_nums, 'Average silhouette score' : sil_scores}) 
        
        summary = generate_clustering_summary(trajectory_file=self.trajectory_file, topology_file=self.topology_file, pka_file=self.pka_file, residues=self.residues, include_distances=self.include_distances, clustering_method=clustering_method, automatic=automatic, silhouette_scores=silhouette_scores, n_components=self.n_components, cummulative_variance=self.cummulative_variance) 
        return write_clustering_info(summary=summary, trajectory_dict=self.trajectory_dict, pka_df=self.pka_df, times=self.times, frames=self.frames, trajectory_names=self.trajectory_names, labels=labels, cluster_centers=cluster_centers, cluster_indices=medoid_indices, cluster_centers_trajectories=cluster_centers_trajectories, log_file=self.log_file, clustering_method=clustering_method) 
    


    def gromos(self, automatic=False, max_cluster_population=0.95, cutoff=0.1, max_clusters=20, max_cutoffs=20): 

        """
        This function implements the gromos method to do the clustering 
        as described by Micheletti et al. 
        (DOI: 10.1002/1097-0134(20000901)40:4<662::aid-prot90>3.0.co;2-f). 
        
        automatic: bool, default=False, if True the clustering is run 
        using various cutoffs (number of cutoffs defined by max_cutoffs). 
        The silhouette score is determined at each iteration and 
        the best cutoff is chosen based on the highest silhouette 
        score. Cutoffs that produce more than the allowed max number 
        of clusters (defined by max_clusters) or that produce only one 
        cluster are excluded. 
        
        cutoff: float, default=0.1, cutoff used for clustering. Ignored 
        if automatic=True. 

        max_clusters: int, default=20, max number of clusters allowed 
        when automatic=True. 

        max_cutoffs: int, default=20, max number of cutoffs used to find 
        the most suitable one for clustering. 
        """
        clustering_method = 'GROMOS' 

        rmsd_matrix = calculate_rmsd_matrix(self.clustering_matrix, self.frames) 

        if automatic == False: 
            labels, cluster_centers, cluster_center_indices, cluster_centers_trajectories = gromos_clustering(cutoff=cutoff, rmsd_matrix=rmsd_matrix, frames=self.frames, trajectory_names=self.trajectory_names) 
            sil_score = round(silhouette_score(self.clustering_matrix, labels), 4) 
            print(f'Clustering with a cutoff of {cutoff} produces {len(set(labels))} clusters with an average silhouette score of {sil_score}.') 
            silhouette_scores = pd.DataFrame({'Number of clusters' : len(set(labels)), 'RMSD cutoff' : cutoff, 'Average silhouette score' : sil_score}) 
        
        elif automatic == True: 
            sil_scores = [] 
            max_rmsd = np.max(rmsd_matrix) 
            step = max_rmsd/max_cutoffs 
            cutoffs = [] 
            cluster_nums = [] 
            for cutoff_i in np.arange(step,max_rmsd+step,step): 
                cutoff_i = round(cutoff_i, 4) 
                labels, cluster_centers, cluster_center_indices, cluster_centers_trajectories = gromos_clustering(cutoff=cutoff_i, rmsd_matrix=rmsd_matrix, frames=self.frames, trajectory_names=self.trajectory_names) 

                if len(labels) == len(set(labels)): 
                    print(f'Cutoff of {cutoff_i} produces no clusters. Moving on to next cutoff')  

                elif len(set(labels)) == 1: 
                    print(f'Cutoff of {cutoff_i} produces only one cluster. Moving on to next cutoff.') 

                elif len(set(labels)) > max_clusters: 
                    print(f'Cutoff of {cutoff_i} produces more than {max_clusters} clusters. Moving on to next cutoff.') 

                elif determine_cluster_population_ratio(labels=labels, max_cluster_population=max_cluster_population, clustering_method=clustering_method) == True: 
                    print(f'Clustering with a cutoff of {cutoff_i} results in the most populated cluster containing more than {max_cluster_population*100}% of frames. Moving on to next cutoff.') 
                
                else: 
                    cutoffs.append(cutoff_i) 
                    sil_score = round(silhouette_score(self.clustering_matrix, labels), 4) 
                    sil_scores.append(sil_score) 
                    cluster_nums.append(len(set(labels))) 
                    print(f'Clustering with a cutoff of {cutoff_i} produces {len(set(labels))} clusters with an average silhouette score of {sil_score}.')

            sil_scores = np.array(sil_scores) 
            best_cutoff_index = np.argmax(sil_scores) 
            cutoff = cutoffs[best_cutoff_index] 
            sil_score = sil_scores[best_cutoff_index] 
            print(f'Best cutoff identified at {cutoff} with an average silhouette score of {sil_score}.') 
            labels, cluster_centers, cluster_center_indices, cluster_centers_trajectories = gromos_clustering(cutoff=cutoff, rmsd_matrix=rmsd_matrix, frames=self.frames, trajectory_names=self.trajectory_names) 

            silhouette_scores = pd.DataFrame({'Number of clusters' : cluster_nums, 'RMSD cutoff' : cutoffs, 'Average silhouette score' : sil_scores}) 
        
        summary = generate_clustering_summary(trajectory_file=self.trajectory_file, topology_file=self.topology_file, pka_file=self.pka_file, residues=self.residues, include_distances=self.include_distances, clustering_method=clustering_method, automatic=automatic, silhouette_scores=silhouette_scores, n_components=self.n_components, cummulative_variance=self.cummulative_variance) 
        return write_clustering_info(summary=summary, trajectory_dict=self.trajectory_dict, pka_df=self.pka_df, times=self.times, frames=self.frames, trajectory_names=self.trajectory_names, labels=labels, cluster_centers=cluster_centers, cluster_indices=cluster_center_indices, cluster_centers_trajectories=cluster_centers_trajectories, log_file=self.log_file, clustering_method=clustering_method) 
    

    
    def dbscan(self, automatic=False, max_cluster_population=0.95, max_clusters=20, eps=0.5, min_samples=5, metric='euclidean', metric_params=None, algorithm='auto', leaf_size=30, p=None, n_jobs=None): 
        
        """
        This function implements the DBSCAN method to do the clustering. 
        
        automatic: bool, default=False, if True the clustering is run 
        using various combinations of eps and min_samples values. The 
        silhouette score is determined at each iteration and 
        the best combination of values is chosen based on the highest 
        silhouette score. Combinations that produce more than the 
        allowed max number of clusters (defined by max_clusters) or 
        that produce only one cluster are excluded. 
        
        cutoff: float, default=0.1, cutoff used for clustering. Ignored 
        if automatic=True. 

        max_clusters: int, default=20, max number of clusters allowed 
        when automatic=True. 

        eps, min_samples, metric, metric_params, algorithm, 
        leaf_size, p, and n_jobs as found in 
        sklearn.cluster.DBSCAN (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html). 
        """

        clustering_method = 'DBSCAN' 

        if automatic == False: 
            labels, cluster_centers, cluster_center_indices, cluster_centers_trajectories = dbscan_clustering(eps=eps, min_samples=min_samples, metric=metric, metric_params=metric_params, algorithm=algorithm, leaf_size=leaf_size, p=p, n_jobs=n_jobs, clustering_matrix=self.clustering_matrix, frames=self.frames, find_centroid=True, trajectory_names=self.trajectory_names) 
            sil_score = round(silhouette_score(self.clustering_matrix, labels), 4)  
            print(f'Clustering with parameters eps={eps} and min_samples={min_samples} produces {len(set(labels))} clusters with an average silhouette score of {sil_score}.') 
            silhouette_scores = pd.DataFrame({'Number of clusters' : [len(set(labels))], 'Epsilon' : [eps], 'Minimum samples' :[min_samples], 'Average silhouette score' : [sil_score]}) 
            
        elif automatic == True: 
            sil_scores = [] 
            params = [] 
            cluster_nums = [] 
            for eps in np.arange(0.005, 0.505, 0.005): 
                eps = round(eps, 4) 
                for min_samples in np.arange(2, 11, 1): 
                    labels = dbscan_clustering(eps=eps, min_samples=min_samples, metric=metric, metric_params=metric_params, algorithm=algorithm, leaf_size=leaf_size, p=p, n_jobs=n_jobs, clustering_matrix=self.clustering_matrix, frames=self.frames, find_centroid=False, trajectory_names=self.trajectory_names)

                    if len(set(labels)) == len(labels): 
                        print(f'Clustering with parameters eps={eps} and min_samples={min_samples} produces no clusters. Moving on to next set of parameters') 

                    elif len(set(labels)) <= 2: 
                        print(f'Clustering with parameters eps={eps} and min_samples={min_samples} produces only one cluster. Moving on to next set of parameters') 

                    elif len(set(labels)) > max_clusters: 
                        print(f'Clustering with parameters eps={eps} and min_samples={min_samples} produces more than {max_clusters} clusters. Moving on to next set of parameters') 
                    
                    elif determine_cluster_population_ratio(labels=labels, max_cluster_population=max_cluster_population, clustering_method=clustering_method) == True: 
                        print(f'Clustering with parameters eps={eps} and min_samples={min_samples} results in the most populated cluster containing more than {max_cluster_population*100}% of frames. Moving on to next st of parameters.') 
                
                    else: 
                        sil_score = round(silhouette_score(self.clustering_matrix, labels), 4) 
                        sil_scores.append(sil_score) 
                        cluster_nums.append(len(set(labels))) 
                        params.append([eps, min_samples]) 
                        print(f'Clustering with parameters eps={eps} and min_samples={min_samples} produces {len(set(labels))} clusters with an average silhouette score of {sil_score}.') 
                        
            sil_scores = np.array(sil_scores) 
            best_params_index = np.argmax(sil_scores) 
            best_eps, best_min_samples = tuple(params[best_params_index]) 
            sil_score = sil_scores[best_params_index] 
            print(f'Best clustering parameters identified at eps={best_eps} and min_samples={best_min_samples} with an average silhouette score of {sil_score}.') 
            labels, cluster_centers, cluster_center_indices, cluster_centers_trajectories = dbscan_clustering(eps=best_eps, min_samples=best_min_samples, metric=metric, metric_params=metric_params, algorithm=algorithm, leaf_size=leaf_size, p=p, n_jobs=n_jobs, clustering_matrix=self.clustering_matrix, frames=self.frames, find_centroid=True, trajectory_names=self.trajectory_names)
            params = np.array(params) 

            silhouette_scores = pd.DataFrame({'Number of clusters' : cluster_nums, 'Epsilon' : params[:,0], 'Minimum samples' : params[:,1], 'Average silhouette score' : sil_scores}) 
        
        summary = generate_clustering_summary(trajectory_file=self.trajectory_file, topology_file=self.topology_file, pka_file=self.pka_file, residues=self.residues, include_distances=self.include_distances, clustering_method=clustering_method, automatic=automatic, silhouette_scores=silhouette_scores, n_components=self.n_components, cummulative_variance=self.cummulative_variance) 
        return write_clustering_info(summary=summary, trajectory_dict=self.trajectory_dict, pka_df=self.pka_df, times=self.times, frames=self.frames, trajectory_names=self.trajectory_names, labels=labels, cluster_centers=cluster_centers, cluster_indices=cluster_center_indices, cluster_centers_trajectories=cluster_centers_trajectories, log_file=self.log_file, clustering_method=clustering_method) 
    
